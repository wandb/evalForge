{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from collections import defaultdict\n",
    "import instructor\n",
    "import weave\n",
    "from set_env import set_env\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal, Union\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-29 12:47:59.946\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mset_env.set_env\u001b[0m:\u001b[36mset_env\u001b[0m:\u001b[36m100\u001b[0m - \u001b[33m\u001b[1m\n",
      "        Unable to set WANDB_API_KEY=WANDB_API_KEY,\n",
      "        not in colab or Secrets not set, not kaggle\n",
      "        or Secrets not set, no .env/dotenv/env file\n",
      "        in the current working dir or parent dirs.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading envfile='/Users/anishshah/Documents/Manual Library/GitHub(1)/improve-evals/.env' with dotenv_values(envfile)\n"
     ]
    }
   ],
   "source": [
    "set_env(\"OPENAI_API_KEY\")\n",
    "set_env(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(openai.OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPoint = Tuple[dict, dict, Literal[0, 1], str, Optional[str], Optional[str]]  # (input, output, annotation, note, human_description_for_task_or_judge, human_description_for_metric_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_task = \"\"\"\n",
    "You are extracting insights from some medical records.\n",
    "The records contain a medical note and a\n",
    "dialogue between a doctor and a patient. You need\n",
    "to extract values for the following: Chief\n",
    "complaint, History of present illness, Physical\n",
    "examination, symptoms experienced by the patient,\n",
    "New medications prescribed or changed, including\n",
    "dosages (N/A if not provided), and Follow-up\n",
    "instructions (N/A if not provided). Your answer\n",
    "should not include any personal identifiable\n",
    "information (PII) such as name, age, gender, or\n",
    "ID. Use \"the patient\" instead of their name, for\n",
    "example. Return your answer as a bullet list,\n",
    "where each bullet is formatted like •chief\n",
    "complaint: xx. If there is no value for the key,\n",
    "the value should be N/A. Keep your response\n",
    "around 150 words (you may have to summarize some\n",
    "extracted values to stay within the word limit).\n",
    "{transcript}\n",
    "\"\"\"\n",
    "medical_metric_details = \"\"\"\n",
    "word count, presence of the six targeted keys, and absence of PII, with the first two implemented via code- based assertions and the last via an LLM evaluator\n",
    "\"\"\"\n",
    "medical_dataset_url = \"https://raw.githubusercontent.com/wyim/aci-bench/main/data/challenge_data/train.csv\"\n",
    "\n",
    "def load_medical_data(url: str) -> List[DataPoint]:\n",
    "    df = pd.read_csv(url)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_task = \"\"\"\n",
    "You are an expert copywriter. You need to write an e-\n",
    "commerce product description based on the product\n",
    "details and customer reviews. Your description\n",
    "should be SEO-optimized. It should use an active\n",
    "voice and include the product's features,\n",
    "benefits, unique selling points without\n",
    "overpromising, and a call to action for the buyer\n",
    "• Benefits describe how product features will\n",
    "work for the buyer, addressing exactly how the\n",
    "product will improve their lives. Clearly\n",
    "distinguish between features (e.g., lightweight,\n",
    "USB-chargeable) and benefits (e.g., convenience,\n",
    "nutritious drinks on-the-go). Don't mention\n",
    "weaknesses of the product or use generic or\n",
    "repetitive language. Don't make up review text or\n",
    "quotes. Don't include any links. Don't cite the\n",
    "reviews too heavily. Divide your description into\n",
    "readable chunks divided by relevant subheadings.\n",
    "Keep your description around 200 words, no more\n",
    "than 300, in Markdown format.\n",
    "{document}\n",
    "\"\"\"\n",
    "product_metric_details = \"\"\"\n",
    "absence of negative reviews, absence of links, adherence to markdown format, and word count limitation, with only the first criterion requiring LLM implementation\n",
    "\"\"\"\n",
    "product_dataset_url = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TASK = \"medical\"\n",
    "if TEST_TASK == \"medical\":\n",
    "    task = medical_task\n",
    "    metric_details = medical_metric_details\n",
    "    data = \n",
    "elif TEST_TASK == \"product\":\n",
    "    task = product_task\n",
    "    metric_details = product_metric_details\n",
    "    dataset_url = product_dataset_url\n",
    "else:\n",
    "    task = None\n",
    "    metric_details = None\n",
    "    dataset_url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ML use cases from call transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ({\"text\": \"Summarize the impact of climate change on polar bears.\"}, {\"text\": \"Climate change is reducing sea ice, which polar bears rely on for hunting seals.\"}, 1, \"Accurate and relevant.\"),\n",
    "    ({\"text\": \"Explain the process of photosynthesis.\"}, {\"text\": \"Photosynthesis is the process by which plants use sunlight to synthesize foods from carbon dioxide and water.\"}, 1, \"Correct and detailed.\"),\n",
    "    ({\"text\": \"What are the main causes of the American Civil War?\"}, {\"text\": \"The main causes were slavery, states' rights, and economic differences.\"}, 1, \"Concise and accurate.\"),\n",
    "    ({\"text\": \"Describe the symptoms of COVID-19.\"}, {\"text\": \"COVID-19 is caused by a virus that originated in bats.\"}, 0, \"Irrelevant and incorrect.\"),\n",
    "    ({\"text\": \"What is the significance of the Magna Carta?\"}, {\"text\": \"The Magna Carta was a document that limited the power of the king and established certain legal rights.\"}, 1, \"Historically accurate and relevant.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data: List[DataPoint]) -> str:\n",
    "    formatted_data = [\n",
    "        f\"{i+1}. Input: {dp[0]['text']}\\n\"\n",
    "        f\"   Output: {dp[1]['text']}\\n\"\n",
    "        f\"   Annotation: {'Correct' if dp[2] == 1 else 'Incorrect'}\\n\"\n",
    "        f\"   Note: {dp[3]}\"\n",
    "        for i, dp in enumerate(data)\n",
    "    ]\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Input: Summarize the impact of climate change on polar bears.\n",
      "   Output: Climate change is reducing sea ice, which polar bears rely on for hunting seals.\n",
      "   Annotation: Correct\n",
      "   Note: Accurate and relevant.\n",
      "\n",
      "2. Input: Explain the process of photosynthesis.\n",
      "   Output: Photosynthesis is the process by which plants use sunlight to synthesize foods from carbon dioxide and water.\n",
      "   Annotation: Correct\n",
      "   Note: Correct and detailed.\n",
      "\n",
      "3. Input: What are the main causes of the American Civil War?\n",
      "   Output: The main causes were slavery, states' rights, and economic differences.\n",
      "   Annotation: Correct\n",
      "   Note: Concise and accurate.\n",
      "\n",
      "4. Input: Describe the symptoms of COVID-19.\n",
      "   Output: COVID-19 is caused by a virus that originated in bats.\n",
      "   Annotation: Incorrect\n",
      "   Note: Irrelevant and incorrect.\n",
      "\n",
      "5. Input: What is the significance of the Magna Carta?\n",
      "   Output: The Magna Carta was a document that limited the power of the king and established certain legal rights.\n",
      "   Annotation: Correct\n",
      "   Note: Historically accurate and relevant.\n"
     ]
    }
   ],
   "source": [
    "#Maintain key names in the formatted_string\n",
    "formatted_data = format_data(data)\n",
    "formatted_data_string = \"\\n\\n\".join(formatted_data)\n",
    "print(formatted_data_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In geenral refinement instead of dumping all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_description(formatted_data: List[str]) -> str:\n",
    "    # Read each attribute from the datapoints individually and then combine to determine the underlying intent of the task and display in a usbale LLM way\n",
    "    # Read inputs\n",
    "    # Read outputs\n",
    "    # Take both learnings into the task description\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_human_and_llm_descriptions(human_description: str, llm_description: str) -> str:\n",
    "    # Combine the human and llm descriptions into a single description\n",
    "    # Use the llm description as the primary description\n",
    "    # Use the human description to fill in any gaps or provide additional context\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationCriteria(BaseModel):\n",
    "    criteria: List[str] = Field(\n",
    "        ...,\n",
    "        min_items=3,\n",
    "        max_items=5,\n",
    "        description=\"List of 3-5 evaluation criteria generated from the annotated data\"\n",
    "    )\n",
    "\n",
    "def generate_criteria(formatted_data_string: str) -> List[str]:\n",
    "    prompt = f\"\"\"\n",
    "Analyze the following annotated data and generate 3-5 evaluation criteria:\n",
    "\n",
    "{formatted_data_string}\n",
    "\n",
    "Your task is to generate evaluation criteria that can be used to assess the quality of outputs for this task. When generating criteria, consider the following guidelines from recent research on LLM evaluation:\n",
    "\n",
    "1. Focus on general aspects of quality that can be used across multiple outputs\n",
    "2. Consider criteria that address potential misalignment between LLM outputs and human preferences\n",
    "3. Include criteria that can be evaluated both by code and by LLM-based evaluators\n",
    "4. Think about criteria that might reveal hallucinations, instruction-following, or other common LLM issues\n",
    "5. Generate criteria that could help in debugging or improving the LLM pipeline\n",
    "\n",
    "Provide each criterion as a concise statement, followed by a brief explanation of why it's important and how it might be evaluated (e.g., via code, LLM evaluator, or human judgment).\n",
    "\n",
    "Return the criteria in this format:\n",
    "[Criterion]: [Brief explanation and evaluation method]\n",
    "[Criterion]: [Brief explanation and evaluation method]\n",
    "[Criterion]: [Brief explanation and evaluation method]\n",
    "\n",
    "Aim for a mix of straightforward, code-evaluable criteria and more nuanced criteria that might require LLM or human evaluation.\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=300,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_model=EvaluationCriteria\n",
    "    )\n",
    "    return response.criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated criteria:\n",
      "1. Relevance: The output should align closely with the input question or prompt, providing information that is directly related. This can be evaluated using keyword matching algorithms or manual human judgment to ensure the response stays on topic.\n",
      "2. Accuracy: The information provided in the output must be factually correct and up-to-date. This can be evaluated by cross-referencing with reliable databases or sources, potentially using an LLM evaluator to assess factual consistency.\n",
      "3. Completeness: The response should fully address all aspects of the input question, not omitting any key components. This could be checked by an LLM for thoroughness or by a rubric used by human evaluators.\n",
      "4. Clarity: The output should be easy to understand and communicated in a clear manner, avoiding unnecessary complexity or ambiguity. This might be evaluated by language models for readability or by human judges for comprehensibility.\n",
      "5. Correctness (for factual prompts): Ensure the output addresses the factual elements correctly, minimizing errors like false associations or hallucinations. This can be cross-checked against factual databases or using specialized algorithms to verify factual integrity.\n"
     ]
    }
   ],
   "source": [
    "# Generate criteria\n",
    "criteria = generate_criteria(formatted_data)\n",
    "print(\"Generated criteria:\")\n",
    "for i, criterion in enumerate(criteria, 1):\n",
    "    print(f\"{i}. {criterion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonAssertion(BaseModel):\n",
    "    text: str\n",
    "    evaluation_type: Literal[\"python\"]\n",
    "\n",
    "class LLMAssertion(BaseModel):\n",
    "    text: str\n",
    "    evaluation_type: Literal[\"llm\"]\n",
    "\n",
    "class CriterionAssertions(BaseModel):\n",
    "    assertions: List[Union[PythonAssertion, LLMAssertion]] = Field(\n",
    "        ...,\n",
    "        min_items=1,\n",
    "        max_items=3,\n",
    "        description=\"Generate 1-3 specific, testable assertions that can be used to evaluate LLM outputs based on the given criterion\"\n",
    "    )\n",
    "\n",
    "def create_candidate_assertions(formatted_data_string: str, criterion: str) -> CriterionAssertions:\n",
    "    prompt = f\"\"\"\n",
    "Given the following evaluation criterion and annotated data, generate 1-3 specific, testable assertions:\n",
    "\n",
    "Criterion: {criterion}\n",
    "\n",
    "Annotated data: {formatted_data_string}\n",
    "\n",
    "Your task is to create assertions that can be used to evaluate LLM outputs based on this criterion. Follow these guidelines:\n",
    "\n",
    "1. Make each assertion clear, concise, and directly related to the criterion\n",
    "2. Specify whether each assertion should be evaluated using Python code or an LLM\n",
    "3. For Python assertions:\n",
    "   - Provide valid Python code that can be executed to evaluate the assertion\n",
    "   - Use 'output' as the variable name for the LLM output being evaluated\n",
    "   - Return True if the assertion passes, False otherwise\n",
    "4. For LLM assertions:\n",
    "   - Provide a clear, detailed prompt for an LLM to evaluate the assertion\n",
    "   - The prompt should guide the LLM to return \"PASS\" or \"FAIL\" based on the evaluation\n",
    "5. Include a mix of positive and negative assertions where appropriate\n",
    "6. Consider edge cases and potential failure modes for the criterion\n",
    "7. Aim for assertions that could be applied across multiple types of outputs\n",
    "\n",
    "Ensure that your assertions are directly evaluable and avoid vague or subjective language. Focus on creating assertions that align with human preferences and can be used to validate the quality of LLM-generated evaluations.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "{{\n",
    "  \"assertions\": [\n",
    "    {{\n",
    "      \"text\": \"Assertion text or code\",\n",
    "      \"evaluation_type\": \"python\" or \"llm\"\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=1000,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_model=CriterionAssertions\n",
    "    )\n",
    "    return response.assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Criterion: Relevance: The output should align closely with the input question or prompt, providing information that is directly related. This can be evaluated using keyword matching algorithms or manual human judgment to ensure the response stays on topic.\n",
      "Candidate assertions:\n",
      "  Type: PYTHON\n",
      "  Code:\n",
      "    Check if the output contains key phrases or terms that directly relate to the input question or prompt. For example, for the input 'Describe the symptoms of COVID-19', check if the terms like 'symptoms', 'fever', 'cough' are present in the output.\n",
      "  Type: LLM\n",
      "  Prompt:\n",
      "    Prompt LLM: 'Given the input and output pair, evaluate whether the output provides relevant and directly related information to the input. Consider if the output by itself adequately addresses the main topic requested in the input prompt.'\n",
      "  Type: PYTHON\n",
      "  Code:\n",
      "    Ensure the output stays on topic by checking if more than 60% of the output sentences contain thematic words found in the input. Return True if this condition is met, False otherwise.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Criterion: Accuracy: The information provided in the output must be factually correct and up-to-date. This can be evaluated by cross-referencing with reliable databases or sources, potentially using an LLM evaluator to assess factual consistency.\n",
      "Candidate assertions:\n",
      "  Type: PYTHON\n",
      "  Code:\n",
      "    # Check if the output contains factual statements related to the input topic by verifying against a reliable external source.\n",
      "    # Using a hypothetical external module 'fact_check' that provides a verify_factual_content function, which returns True if content is factual.\n",
      "    \n",
      "    from fact_check import verify_factual_content\n",
      "    \n",
      "    assert verify_factual_content(output), 'The content must be factually correct and relevant to the topic.'\n",
      "  Type: LLM\n",
      "  Prompt:\n",
      "    Evaluate whether the information in the output is accurate and relevant based on the following input: '{{input}}'. Use reliable online resources to determine if the claims made are factually correct and up-to-date. Respond with 'PASS' if the information is accurate and relevant, otherwise respond with 'FAIL'.\n",
      "  Type: PYTHON\n",
      "  Code:\n",
      "    # Test if the output correctly addresses the prompt by ensuring it doesn't contain known factual errors or irrelevant information.\n",
      "    # Assuming access to a pre-existing list of known errors or common misconceptions for various topics.\n",
      "    known_errors = { 'COVID-19': \"COVID-19 is caused by a virus that originated in bats\", ... }\n",
      "    \n",
      "    assert all(error not in output for error in known_errors.values()), 'Output contains factual errors or irrelevant information.'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Criterion: Completeness: The response should fully address all aspects of the input question, not omitting any key components. This could be checked by an LLM for thoroughness or by a rubric used by human evaluators.\n",
      "Candidate assertions:\n",
      "  Type: LLM\n",
      "  Prompt:\n",
      "    Check if the response includes all major components of the requested topic, such as key factors, processes, or historical impacts related to the input question. Use the input to determine the scope and ensure nothing critical is missed.\n",
      "  Type: PYTHON\n",
      "  Code:\n",
      "    output = 'polar bears' in output.lower() and 'sea ice' in output.lower() and 'hunting' in output.lower()\\nreturn output\n",
      "  Type: LLM\n",
      "  Prompt:\n",
      "    For the given input, evaluate whether the response thoroughly covers each aspect of the question without omitting key components, and provide an explanation for any omissions.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Criterion: Clarity: The output should be easy to understand and communicated in a clear manner, avoiding unnecessary complexity or ambiguity. This might be evaluated by language models for readability or by human judges for comprehensibility.\n",
      "Candidate assertions:\n",
      "  Type: PYTHON\n",
      "  Code:\n",
      "    output_contains_key_points = all(key in output for key in ['sea ice', 'plants use sunlight', 'slavery', 'legal rights'])\n",
      "    return output_contains_key_points\n",
      "  Type: LLM\n",
      "  Prompt:\n",
      "    Evaluate the following output for clarity and conciseness. Consider if it avoids unnecessary complexity or ambiguity and directly answers the input question with relevant information. Return 'PASS' if it is clear and concise, otherwise return 'FAIL'.\n",
      "  Type: PYTHON\n",
      "  Code:\n",
      "    output_length_within_range = 50 <= len(output.split()) <= 150\n",
      "    return output_length_within_range\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Criterion: Correctness (for factual prompts): Ensure the output addresses the factual elements correctly, minimizing errors like false associations or hallucinations. This can be cross-checked against factual databases or using specialized algorithms to verify factual integrity.\n",
      "Candidate assertions:\n",
      "  Type: LLM\n",
      "  Prompt:\n",
      "    Ensure that the output includes accurate, factual elements as required by the prompt. Use factual databases or external verification tools to confirm the correctness.\n",
      "  Type: PYTHON\n",
      "  Code:\n",
      "    output_correct = ('polar bears' in output and 'sea ice' in output and 'hunting seals' in output) or ('photosynthesis' in output and 'sunlight' in output and 'carbon dioxide' in output and 'water' in output) or ('slavery' in output and 'states\\' rights' in output and 'economic differences' in output) or ('Magna Carta' in output and 'limited the power' in output and 'legal rights' in output)\n",
      "    return output_correct\n",
      "  Type: LLM\n",
      "  Prompt:\n",
      "    Check if the LLM output includes hallucinations or false associations by comparing the output with recognized factual databases. If any discrepancies are found, the output should fail.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create candidate assertions\n",
    "for criterion in criteria:\n",
    "    assertions = create_candidate_assertions(formatted_data, criterion)\n",
    "    print(f\"\\nCriterion: {criterion}\")\n",
    "    print(\"Candidate assertions:\")\n",
    "    for assertion in assertions:\n",
    "        print(f\"  Type: {assertion.evaluation_type.upper()}\")\n",
    "        if assertion.evaluation_type == 'python':\n",
    "            print(\"  Code:\")\n",
    "            print(\"    \" + assertion.text.replace('\\n', '\\n    '))\n",
    "        else:\n",
    "            print(\"  Prompt:\")\n",
    "            print(\"    \" + assertion.text.replace('\\n', '\\n    '))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_python_assertion(assertion, datum):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_assertion(assertion, datum):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_assertions(datum, assertion):\n",
    "    if assertion.evaluation_type == 'python':\n",
    "        return evaluate_python_assertion(assertion.text, datum)\n",
    "    else:\n",
    "        return evaluate_llm_assertion(assertion.text, datum)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate assertions\n",
    "assertion_results = evaluate_assertions(data, assertions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Selectivity**:\n",
    "   ```python\n",
    "   selectivity = passes / total_outputs\n",
    "   ```\n",
    "   Selectivity measures how often an assertion passes LLM outputs. A lower selectivity means the assertion is more \"picky\" or strict.\n",
    "\n",
    "2. **Coverage**:\n",
    "   ```python\n",
    "   coverage = fails_on_bad / total_bad if total_bad > 0 else 0\n",
    "   ```\n",
    "   Coverage measures how well our assertions catch the outputs that humans marked as bad. A higher coverage means we're better at identifying problematic outputs.\n",
    "\n",
    "3. **False Failure Rate (FFR)**:\n",
    "   ```python\n",
    "   ffr = fails_on_good / total_good if total_good > 0 else 0\n",
    "   ```\n",
    "   FFR shows how often our assertions incorrectly fail outputs that humans thought were good. A lower FFR is better, as it means we're not being overly strict.\n",
    "\n",
    "4. **Alignment**:\n",
    "   ```python\n",
    "   alignment = 2 * (coverage * (1 - ffr)) / (coverage + (1 - ffr)) if (coverage + (1 - ffr)) > 0 else 0\n",
    "   ```\n",
    "   Alignment combines coverage and FFR into a single score. It represents how well our automated evaluations match human judgments overall.\n",
    "\n",
    "These metrics help us refine our assertion set over time, aiming to catch more bad outputs while avoiding false alarms on good ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get Alignment score right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data: List[DataPoint], assertion_results: Dict[str, List[int]]) -> Dict[str, Dict[str, float]]:\n",
    "    metrics = {}\n",
    "    total_outputs = len(data)\n",
    "    total_bad = sum(1 for _, _, annotation, _ in data if annotation == 0)\n",
    "    total_good = total_outputs - total_bad\n",
    "\n",
    "    for assertion, results in assertion_results.items():\n",
    "        passes = sum(results)\n",
    "        fails_on_bad = sum(1 for (_, _, annotation), result in zip(data, results) if annotation == 0 and result == 0)\n",
    "        fails_on_good = sum(1 for (_, _, annotation), result in zip(data, results) if annotation == 1 and result == 0)\n",
    "\n",
    "        selectivity = passes / total_outputs\n",
    "        coverage = fails_on_bad / total_bad if total_bad > 0 else 0\n",
    "        ffr = fails_on_good / total_good if total_good > 0 else 0\n",
    "        alignment = 2 * (coverage * (1 - ffr)) / (coverage + (1 - ffr)) if (coverage + (1 - ffr)) > 0 else 0\n",
    "        # human_accuracy = \n",
    "\n",
    "        metrics[assertion] = {\n",
    "            \"selectivity\": selectivity,\n",
    "            \"coverage\": coverage,\n",
    "            \"ffr\": ffr,\n",
    "            \"alignment\": alignment\n",
    "        }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "metrics = calculate_metrics(data, assertion_results)\n",
    "print(\"Assertion metrics:\", json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerthink this especially for python case because unittest may not always be pure 1 assertion test (like a test suite)\n",
    "def select_best_assertions(assertions: Dict[str, List[str]], metrics: Dict[str, Dict[str, float]]) -> Dict[str, str]:\n",
    "    best_assertions = {}\n",
    "    for criterion, assertion_list in assertions.items():\n",
    "        best_assertion = max(assertion_list, key=lambda a: metrics[a]['alignment'])\n",
    "        best_assertions[criterion] = best_assertion\n",
    "    return best_assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best assertions\n",
    "best_assertions = select_best_assertions(assertions, metrics)\n",
    "print(\"Best assertions:\", json.dumps(best_assertions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_metrics(data: List[DataPoint], best_assertions: Dict[str, str], assertion_results: Dict[str, List[int]]) -> Dict[str, float]:\n",
    "    total_outputs = len(data)\n",
    "    total_bad = sum(1 for _, _, annotation, _ in data if annotation == 0)\n",
    "    total_good = total_outputs - total_bad\n",
    "\n",
    "    fails_on_bad = sum(1 for i, (_, _, annotation, _) in enumerate(data) \n",
    "                       if annotation == 0 and any(assertion_results[assertion][i] == 0 for assertion in best_assertions.values()))\n",
    "    fails_on_good = sum(1 for i, (_, _, annotation, _) in enumerate(data) \n",
    "                        if annotation == 1 and any(assertion_results[assertion][i] == 0 for assertion in best_assertions.values()))\n",
    "\n",
    "    coverage = fails_on_bad / total_bad if total_bad > 0 else 0\n",
    "    ffr = fails_on_good / total_good if total_good > 0 else 0\n",
    "    alignment = 2 * (coverage * (1 - ffr)) / (coverage + (1 - ffr)) if (coverage + (1 - ffr)) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"coverage\": coverage,\n",
    "        \"ffr\": ffr,\n",
    "        \"alignment\": alignment\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "overall_metrics = calculate_overall_metrics(data, best_assertions, assertion_results)\n",
    "print(\"Overall metrics:\", json.dumps(overall_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "report = {\n",
    "    \"final_assertions\": best_assertions,\n",
    "    \"assertion_metrics\": {assertion: metrics[assertion] for assertion in best_assertions.values()},\n",
    "    \"overall_metrics\": overall_metrics\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Report:\")\n",
    "print(json.dumps(report, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
