{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from collections import defaultdict\n",
    "import instructor\n",
    "import weave\n",
    "from set_env import set_env\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal, Union, Any\n",
    "from pprint import pprint\n",
    "from instructor_models import TaskDescription, CombinedTaskDescription, Criterion, EvaluationCriteria, PythonAssertion, LLMAssertion, CriterionAssertions\n",
    "import asyncio\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env(\"OPENAI_API_KEY\")\n",
    "set_env(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import IPython\n",
    "    in_jupyter = True\n",
    "except ImportError:\n",
    "    in_jupyter = False\n",
    "if in_jupyter:\n",
    "    nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "weave.init(f\"evalgen_test_{random.randint(0, 1000000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(openai.AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPoint = Tuple[dict, dict, Literal[0, 1], Optional[str], Optional[str], Optional[str]]  # (input, output, annotation, note, human_description_for_task_or_judge, human_description_for_metric_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TASK = \"medical\"\n",
    "if TEST_TASK == \"medical\":\n",
    "    data = weave.ref(\"weave:///a-sh0ts/medical_data_results/object/medical_data_annotations:4utHXhRnO2oquowlrvJxCztretSxtFavBUHVciMZJBw\").get()\n",
    "elif TEST_TASK == \"product\":\n",
    "    pass\n",
    "else:\n",
    "    data = [\n",
    "        ({\"text\": \"Summarize the impact of climate change on polar bears.\"}, {\"text\": \"Climate change is reducing sea ice, which polar bears rely on for hunting seals.\"}, 1, \"Accurate and relevant.\"),\n",
    "        ({\"text\": \"Explain the process of photosynthesis.\"}, {\"text\": \"Photosynthesis is the process by which plants use sunlight to synthesize foods from carbon dioxide and water.\"}, 1, \"Correct and detailed.\"),\n",
    "        ({\"text\": \"What are the main causes of the American Civil War?\"}, {\"text\": \"The main causes were slavery, states' rights, and economic differences.\"}, 1, \"Concise and accurate.\"),\n",
    "        ({\"text\": \"Describe the symptoms of COVID-19.\"}, {\"text\": \"COVID-19 is caused by a virus that originated in bats.\"}, 0, \"Irrelevant and incorrect.\"),\n",
    "        ({\"text\": \"What is the significance of the Magna Carta?\"}, {\"text\": \"The Magna Carta was a document that limited the power of the king and established certain legal rights.\"}, 1, \"Historically accurate and relevant.\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Batch this as opposed to one at a time\n",
    "# or sample the dataset and ensure that taking into tokens (maybe something fun with a distribution)\n",
    "# distribution = more stuff we can grab and throw into prompt in smart way\n",
    "@weave.op()\n",
    "async def get_task_description(data: List[DataPoint]) -> str:\n",
    "    task_description = \"\"\n",
    "    \n",
    "    for i, datapoint in enumerate(data):\n",
    "        input_data, output_data, annotation, note = datapoint[0], datapoint[1], datapoint[2], datapoint[3]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Current task description: {task_description}\n",
    "\n",
    "        New datapoint:\n",
    "        Input: {input_data}\n",
    "        Output: {output_data}\n",
    "        Annotation: {\"Correct\" if annotation == 1 else \"Incorrect\"}\n",
    "        Note: {note}\n",
    "\n",
    "        Based on this new datapoint and the current task description, provide an updated, more refined task description. \n",
    "        If this is the first datapoint, create an initial task description.\n",
    "        Focus on:\n",
    "        1. The nature of the input and output data\n",
    "        2. The specific information being extracted or transformed\n",
    "        3. Any formatting or style requirements\n",
    "        4. Evaluation criteria (based on the annotation and note)\n",
    "\n",
    "        Keep the description concise yet comprehensive.\n",
    "        \"\"\"\n",
    "\n",
    "        response = await client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_model=TaskDescription\n",
    "        )\n",
    "        \n",
    "        new_description = response.description\n",
    "        \n",
    "        # TODO: Add guardrails to prevent LLM from saying no update needed\n",
    "        if new_description.lower().startswith(\"no update needed\"):\n",
    "            continue\n",
    "        \n",
    "        task_description = new_description\n",
    "\n",
    "    return task_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_task_description = await get_task_description(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def combine_human_and_llm_descriptions(data: List[DataPoint], llm_description: str) -> str:\n",
    "    human_descriptions = set()\n",
    "    for dp in data:\n",
    "        if len(dp) > 4 and dp[4]:  # Check if human description exists\n",
    "            human_descriptions.add(dp[4])\n",
    "    \n",
    "    if not human_descriptions:\n",
    "        return llm_description\n",
    "    \n",
    "    human_context = \"\\n\".join(f\"- {desc}\" for desc in human_descriptions)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    LLM-generated task description:\n",
    "    {llm_description}\n",
    "\n",
    "    Additional human-provided context:\n",
    "    {human_context}\n",
    "\n",
    "    Your task is to create a comprehensive, coherent task description that combines insights from both the LLM-generated description and the human-provided context. Ensure that:\n",
    "    1. The final description is clear and concise.\n",
    "    2. It incorporates key points from both sources.\n",
    "    3. Any contradictions are resolved logically.\n",
    "    4. The description maintains a professional tone.\n",
    "    5. It provides a complete picture of the task requirements and evaluation criteria.\n",
    "\n",
    "    Please provide the combined description in a single, well-structured paragraph.\n",
    "    \"\"\"\n",
    "\n",
    "    response = await client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_model=CombinedTaskDescription\n",
    "    )\n",
    "    \n",
    "    return response.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_task_description = await combine_human_and_llm_descriptions(data, llm_task_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_task_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_single_datapoint(dp: DataPoint, finalized_task_description: str) -> str:\n",
    "    input_data, output_data, annotation, note = dp[0], dp[1], dp[2], dp[3]\n",
    "    metrics_details = dp[5] if len(dp) > 5 else None\n",
    "\n",
    "    formatted = [\n",
    "        f\"Task Description: {finalized_task_description}\",\n",
    "        \"\",\n",
    "        \"Input:\",\n",
    "        \"\\n\".join(f\"  {key.capitalize()}: {value}\" for key, value in input_data.items()),\n",
    "        \"\",\n",
    "        \"Output:\",\n",
    "        \"\\n\".join(f\"  {key.capitalize()}: {value}\" for key, value in output_data.items()),\n",
    "        \"\",\n",
    "        f\"Annotation: {'Correct' if annotation == 1 else 'Incorrect'}\",\n",
    "        f\"Note: {note}\"\n",
    "    ]\n",
    "\n",
    "    if metrics_details:\n",
    "        formatted.append(f\"Metrics Details: {metrics_details}\")\n",
    "\n",
    "    return \"\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dp = format_single_datapoint(data[0], finalized_task_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(formatted_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def process_criteria(formatted_dp: str, finalized_task_description: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "Task Description: {finalized_task_description}\n",
    "\n",
    "Analyze the following annotated datapoint:\n",
    "\n",
    "{formatted_dp}\n",
    "\n",
    "Generate 1-3 evaluation criteria that can be used to assess the quality of outputs for this task. Consider the following guidelines:\n",
    "\n",
    "1. If a 'Metrics Details' field is present in the datapoint, prioritize this information as it provides the most important evaluation criteria.\n",
    "2. Focus on general aspects of quality that can be used across multiple outputs.\n",
    "3. Consider criteria that address potential misalignment between LLM outputs and human preferences.\n",
    "4. Include criteria that can be evaluated both by code and by LLM-based evaluators.\n",
    "5. Think about criteria that might reveal hallucinations, instruction-following, or other common LLM issues.\n",
    "6. Generate criteria that could help in debugging or improving the LLM pipeline.\n",
    "\n",
    "Provide each criterion as a concise statement, followed by a brief explanation of why it's important and how it might be evaluated (e.g., via code, LLM evaluator, or human judgment).\n",
    "\n",
    "Return the criteria in this format:\n",
    "[Criterion]: [Brief explanation and evaluation method]\n",
    "[Criterion]: [Brief explanation and evaluation method]\n",
    "[Criterion]: [Brief explanation and evaluation method]\n",
    "\n",
    "Aim for a mix of straightforward, code-evaluable criteria and more nuanced criteria that might require LLM or human evaluation.\n",
    "\"\"\"\n",
    "    response = await client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_model=EvaluationCriteria\n",
    "    )\n",
    "    return response\n",
    "\n",
    "@weave.op()\n",
    "async def generate_criteria(data: List[DataPoint], finalized_task_description: str) -> List[Criterion]:\n",
    "    all_criteria = []\n",
    "\n",
    "    async def process_datapoint(dp):\n",
    "        formatted_dp = format_single_datapoint(dp, finalized_task_description)\n",
    "        response = (await process_criteria(formatted_dp, finalized_task_description)).criteria\n",
    "        return response\n",
    "\n",
    "    tasks = [process_datapoint(dp) for dp in data]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for new_criteria in results:\n",
    "        # TODO: Add an additional check to see if a nearly identical criterion is already in the list\n",
    "        all_criteria.extend(new_criteria)\n",
    "    \n",
    "\n",
    "\n",
    "    return all_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate criteria\n",
    "criteria = await generate_criteria(data, finalized_task_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def create_candidate_assertions(formatted_data_string: str, criterion: Criterion) -> CriterionAssertions:\n",
    "    prompt = f\"\"\"\n",
    "Given the following evaluation criterion and annotated data, generate 1-3 specific, testable assertions:\n",
    "\n",
    "Criterion: {criterion.dict()}\n",
    "\n",
    "Annotated data: {formatted_data_string}\n",
    "\n",
    "Your task is to create assertions that can be used to evaluate LLM outputs based on this criterion. Follow these guidelines:\n",
    "\n",
    "1. Make each assertion clear, concise, and directly related to the criterion\n",
    "2. For Python assertions:\n",
    "   - Provide a valid Python method that can be used within a unittest.TestCase class\n",
    "   - Ensure the method name is in snake case and starts with test_\n",
    "   - The method should take 'self' as the only input, where 'self.output' is a dictionary containing the LLM output being evaluated\n",
    "   - The 'self.output' dictionary will have the same keys and shape as the output in the annotated data\n",
    "   - Use unittest assertion methods (e.g., self.assertTrue, self.assertEqual) to test the output\n",
    "   - The test should pass if the assertion is met, and fail otherwise\n",
    "   - Only use the keys and shapes present in the annotated data output for your assertions\n",
    "3. For LLM assertions:\n",
    "   - Provide a clear, detailed prompt for an LLM to evaluate the assertion\n",
    "   - The prompt should guide the LLM to return \"PASS\" or \"FAIL\" based on the evaluation\n",
    "4. Include a mix of positive and negative assertions where appropriate\n",
    "5. Consider edge cases and potential failure modes for the criterion\n",
    "6. Aim for assertions that could be applied across multiple types of outputs\n",
    "\n",
    "Ensure that your assertions are directly evaluable and avoid vague or subjective language. Focus on creating assertions that align with human preferences and can be used to validate the quality of LLM-generated evaluations.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "{{\n",
    "  \"assertions\": [\n",
    "    {{\n",
    "      \"test_name\": \"Name of the test case method in snake case\",\n",
    "      \"text\" or \"code\": \"Assertion text or code\",\n",
    "      \"evaluation_type\": \"python\" or \"llm\"\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "    response = await client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_model=CriterionAssertions\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: improve this function\n",
    "def format_all_datapoints(data: List[DataPoint], finalized_task_description: str) -> str:\n",
    "    formatted = [f\"Task Description: {finalized_task_description}\\n\"]\n",
    "    \n",
    "    for i, dp in enumerate(data, 1):\n",
    "        input_data, output_data, annotation, note = dp[0], dp[1], dp[2], dp[3]\n",
    "        \n",
    "        formatted.extend([\n",
    "            f\"Example {i}:\",\n",
    "            \"Input:\",\n",
    "            json.dumps(input_data, indent=2),\n",
    "            \"\",\n",
    "            \"Output:\",\n",
    "            json.dumps(output_data, indent=2),\n",
    "            \"\",\n",
    "            f\"Annotation: {'Correct' if annotation == 1 else 'Incorrect'}\",\n",
    "            f\"Note: {note}\",\n",
    "            \"\\n\" + \"-\"*50 + \"\\n\"  # Separator between examples\n",
    "        ])\n",
    "    \n",
    "    return \"\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = format_all_datapoints(data, finalized_task_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def generate_all_assertions(criteria, formatted_data):\n",
    "    all_assertions = []\n",
    "\n",
    "    async def process_criterion(criterion):\n",
    "        assertions = (await create_candidate_assertions(formatted_data, criterion)).assertions\n",
    "        return assertions\n",
    "\n",
    "    # Create tasks for all criteria\n",
    "    tasks = [process_criterion(criterion) for criterion in criteria]\n",
    "\n",
    "    # Use asyncio.gather to run all tasks concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for assertions in results:\n",
    "        all_assertions.extend(assertions)\n",
    "\n",
    "    return all_assertions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "all_assertions = await generate_all_assertions(criteria, formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[assertion for assertion in all_assertions if isinstance(assertion, PythonAssertion)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[assertion for assertion in all_assertions if isinstance(assertion, LLMAssertion)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datapoint_to_example(task_description: str, data: List[DataPoint]) -> List[Dict[str, Any]]:\n",
    "    examples = []\n",
    "    for dp in data:\n",
    "        input_data, output_data, annotation, note = dp[0], dp[1], dp[2], dp[3]\n",
    "        examples.append({\n",
    "            \"task_description\": task_description,\n",
    "            \"input_data\": input_data,\n",
    "            \"model_output\": {\"output\": output_data},\n",
    "            \"annotation\": annotation,\n",
    "            \"note\": note\n",
    "        })\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_examples = convert_datapoint_to_example(finalized_task_description, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from combined_scorer import AssertionScorer, predict_passthrough\n",
    "\n",
    "# Initialize the AssertionScorer with the assertions\n",
    "scorer = AssertionScorer(\n",
    "    assertions=all_assertions,\n",
    "    llm_model=\"gpt-4o-2024-08-06\",\n",
    "    prompt_template=\"\"\"\n",
    "Task Description:\n",
    "{task_description}\n",
    "\n",
    "Evaluate the following output based on the given task, input, and assertion:\n",
    "\n",
    "Input:\n",
    "{input_data}\n",
    "\n",
    "Output:\n",
    "{model_output}\n",
    "\n",
    "Assertion:\n",
    "{assertion_text}\n",
    "\n",
    "Consider the task description and input when evaluating the output against the assertion.\n",
    "Respond with either 'PASS' if the output meets the assertion criteria in the context of the task and input, or 'FAIL' if it does not.\n",
    "\"\"\",\n",
    "    system_prompt=\"You are an AI assistant evaluating the quality of text outputs based on given tasks, inputs, and assertions.\"\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: figure out how to get each examples individual results as opposed to aggregate\n",
    "# Create a custom summarize function?\n",
    "evaluation = weave.Evaluation(\n",
    "    scorers=[scorer],\n",
    "    dataset=annotation_examples,\n",
    ")\n",
    "\n",
    "\n",
    "assertion_results = asyncio.run(evaluation.evaluate(predict_passthrough))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def evaluate(scorer: AssertionScorer, annotation_examples: List[Dict[str, Any]]) -> Dict[str, List[Tuple[int, int, str]]]:\n",
    "    async def process_example(example):\n",
    "        result = await scorer.score(\n",
    "            model_output={\"output\": example[\"model_output\"][\"output\"]},\n",
    "            task_description=example[\"task_description\"],\n",
    "            input_data=example[\"input_data\"]\n",
    "        )\n",
    "        return result, example[\"annotation\"]\n",
    "\n",
    "    # Run all examples concurrently\n",
    "    results = await asyncio.gather(*[process_example(example) for example in annotation_examples])\n",
    "\n",
    "    # Initialize the result dictionary\n",
    "    assertion_results: Dict[str, List[Tuple[int, int, str]]] = {}\n",
    "\n",
    "    # Populate the result dictionary\n",
    "    for result, human_annotation in results:\n",
    "        llm_results = result.get('llm_assertion_results', {})\n",
    "        code_results = result.get('code_assertion_results', {}).get('test_results', {})\n",
    "        \n",
    "        for assertion_name, score in llm_results.items():\n",
    "            if assertion_name not in assertion_results:\n",
    "                assertion_results[assertion_name] = []\n",
    "            assertion_results[assertion_name].append((score, human_annotation, \"llm\"))\n",
    "        \n",
    "        for assertion_name, details in code_results.items():\n",
    "            if assertion_name.endswith('_score'):\n",
    "                base_name = assertion_name[:-6]  # Remove '_score' suffix\n",
    "                if base_name not in assertion_results:\n",
    "                    assertion_results[base_name] = []\n",
    "                assertion_results[base_name].append((details, human_annotation, \"python\"))\n",
    "\n",
    "    return assertion_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_results = asyncio.run(evaluate(scorer, annotation_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Selectivity**:\n",
    "   ```python\n",
    "   selectivity = passes / total_outputs\n",
    "   ```\n",
    "   Selectivity measures how often an assertion passes LLM outputs. A lower selectivity means the assertion is more \"picky\" or strict.\n",
    "\n",
    "2. **Coverage**:\n",
    "   ```python\n",
    "   coverage = fails_on_bad / total_bad if total_bad > 0 else 0\n",
    "   ```\n",
    "   Coverage measures how well our assertions catch the outputs that humans marked as bad. A higher coverage means we're better at identifying problematic outputs.\n",
    "\n",
    "3. **False Failure Rate (FFR)**:\n",
    "   ```python\n",
    "   ffr = fails_on_good / total_good if total_good > 0 else 0\n",
    "   ```\n",
    "   FFR shows how often our assertions incorrectly fail outputs that humans thought were good. A lower FFR is better, as it means we're not being overly strict.\n",
    "\n",
    "4. **Alignment**:\n",
    "   ```python\n",
    "   alignment = 2 * (coverage * (1 - ffr)) / (coverage + (1 - ffr)) if (coverage + (1 - ffr)) > 0 else 0\n",
    "   ```\n",
    "   Alignment combines coverage and FFR into a single score. It represents how well our automated evaluations match human judgments overall.\n",
    "\n",
    "These metrics help us refine our assertion set over time, aiming to catch more bad outputs while avoiding false alarms on good ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(assertion_results: Dict[str, List[Tuple[int, int, str]]]) -> Dict[str, Dict[str, Union[float, str]]]:\n",
    "    metrics = {}\n",
    "    \n",
    "    for assertion, results in assertion_results.items():\n",
    "        total_outputs = len(results)\n",
    "        total_bad = sum(1 for _, human_annotation, _ in results if human_annotation == 0)\n",
    "        total_good = total_outputs - total_bad\n",
    "\n",
    "        passes = sum(1 for score, _, _ in results if score == 1)\n",
    "        fails = total_outputs - passes\n",
    "        fails_on_bad = sum(1 for score, human_annotation, _ in results if human_annotation == 0 and score == 0)\n",
    "        fails_on_good = sum(1 for score, human_annotation, _ in results if human_annotation == 1 and score == 0)\n",
    "\n",
    "        selectivity = passes / total_outputs if total_outputs > 0 else 0\n",
    "        coverage = fails_on_bad / total_bad if total_bad > 0 else 1  # If no bad outputs, perfect coverage\n",
    "        ffr = fails_on_good / total_good if total_good > 0 else 0  # If no good outputs, no false failures\n",
    "\n",
    "        # Calculate alignment\n",
    "        if coverage + (1 - ffr) > 0:\n",
    "            alignment = 2 * (coverage * (1 - ffr)) / (coverage + (1 - ffr))\n",
    "        else:\n",
    "            alignment = 0\n",
    "\n",
    "        # Get the evaluation type (assuming it's consistent for all results of this assertion)\n",
    "        eval_type = results[0][2] if results else \"unknown\"\n",
    "\n",
    "        metrics[assertion] = {\n",
    "            \"type\": eval_type,\n",
    "            \"selectivity\": selectivity,\n",
    "            \"coverage\": coverage,\n",
    "            \"ffr\": ffr,\n",
    "            \"alignment\": alignment,\n",
    "            \"total_outputs\": total_outputs,\n",
    "            \"total_good\": total_good,\n",
    "            \"total_bad\": total_bad,\n",
    "            \"passes\": passes,\n",
    "            \"fails\": fails,\n",
    "            \"fails_on_bad\": fails_on_bad,\n",
    "            \"fails_on_good\": fails_on_good\n",
    "        }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "metrics = calculate_metrics(assertion_results)\n",
    "print(\"Assertion metrics:\", json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_assertions(\n",
    "    metrics: Dict[str, Dict[str, Union[float, str]]],\n",
    "    num_llm_tests: int = None,\n",
    "    num_code_tests: int = None,\n",
    "    alignment_threshold: float = None\n",
    ") -> Dict[str, str]:\n",
    "    best_assertions = {}\n",
    "    \n",
    "    # First, filter assertions based on the alignment threshold\n",
    "    if alignment_threshold is not None:\n",
    "        filtered_metrics = {\n",
    "            a: m for a, m in metrics.items() \n",
    "            if m['alignment'] >= alignment_threshold\n",
    "        }\n",
    "    else:\n",
    "        filtered_metrics = metrics\n",
    "    \n",
    "    # Separate assertions by type\n",
    "    llm_assertions = [a for a, m in filtered_metrics.items() if m['type'] == 'llm']\n",
    "    code_assertions = [a for a, m in filtered_metrics.items() if m['type'] == 'python']\n",
    "    \n",
    "    # Sort assertions by alignment score\n",
    "    llm_assertions.sort(key=lambda a: filtered_metrics[a]['alignment'], reverse=True)\n",
    "    code_assertions.sort(key=lambda a: filtered_metrics[a]['alignment'], reverse=True)\n",
    "    \n",
    "    # Select top N assertions for each type\n",
    "    if num_llm_tests is not None:\n",
    "        best_assertions.update({a: 'llm' for a in llm_assertions[:num_llm_tests]})\n",
    "    \n",
    "    if num_code_tests is not None:\n",
    "        best_assertions.update({a: 'python' for a in code_assertions[:num_code_tests]})\n",
    "    \n",
    "    # If no criteria provided or no assertions selected, select the best assertion overall\n",
    "    if not best_assertions and filtered_metrics:\n",
    "        best_assertion = max(filtered_metrics.keys(), key=lambda a: filtered_metrics[a]['alignment'])\n",
    "        best_assertions[best_assertion] = filtered_metrics[best_assertion]['type']\n",
    "    \n",
    "    return best_assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_assertions = select_best_assertions(\n",
    "    metrics,\n",
    "    num_llm_tests=2,\n",
    "    num_code_tests=1,\n",
    "    alignment_threshold=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Also add filters based on criteria so no two assertions solve the same criteria\n",
    "best_assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_assertion_details(best_assertions: Dict[str, str], all_assertions: List[Union[PythonAssertion, LLMAssertion]]) -> List[Union[PythonAssertion, LLMAssertion]]:\n",
    "    best_assertion_details = []\n",
    "    \n",
    "    for assertion_name, assertion_type in best_assertions.items():\n",
    "        matching_assertions = [\n",
    "            assertion for assertion in all_assertions\n",
    "            if assertion.test_name == assertion_name and assertion.evaluation_type == assertion_type\n",
    "        ]\n",
    "        \n",
    "        if matching_assertions:\n",
    "            best_assertion_details.append(matching_assertions[0])\n",
    "        else:\n",
    "            print(f\"Warning: No matching assertion found for {assertion_name} of type {assertion_type}\")\n",
    "    \n",
    "    return best_assertion_details\n",
    "\n",
    "# Usage\n",
    "best_assertion_details = get_best_assertion_details(best_assertions, all_assertions)\n",
    "\n",
    "# Print the details of the best assertions\n",
    "for assertion in best_assertion_details:\n",
    "    print(f\"Test Name: {assertion.test_name}\")\n",
    "    print(f\"Evaluation Type: {assertion.evaluation_type}\")\n",
    "    if isinstance(assertion, PythonAssertion):\n",
    "        print(f\"Code:\\n{assertion.code}\")\n",
    "    elif isinstance(assertion, LLMAssertion):\n",
    "        print(f\"Text: {assertion.text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_assertion_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_overall_metrics(data: List[DataPoint], best_assertions: Dict[str, str], assertion_results: Dict[str, List[int]]) -> Dict[str, float]:\n",
    "#     total_outputs = len(data)\n",
    "#     total_bad = sum(1 for _, _, annotation, _ in data if annotation == 0)\n",
    "#     total_good = total_outputs - total_bad\n",
    "\n",
    "#     fails_on_bad = sum(1 for i, (_, _, annotation, _) in enumerate(data) \n",
    "#                        if annotation == 0 and any(assertion_results[assertion][i] == 0 for assertion in best_assertions.values()))\n",
    "#     fails_on_good = sum(1 for i, (_, _, annotation, _) in enumerate(data) \n",
    "#                         if annotation == 1 and any(assertion_results[assertion][i] == 0 for assertion in best_assertions.values()))\n",
    "\n",
    "#     coverage = fails_on_bad / total_bad if total_bad > 0 else 0\n",
    "#     ffr = fails_on_good / total_good if total_good > 0 else 0\n",
    "#     alignment = 2 * (coverage * (1 - ffr)) / (coverage + (1 - ffr)) if (coverage + (1 - ffr)) > 0 else 0\n",
    "\n",
    "#     return {\n",
    "#         \"coverage\": coverage,\n",
    "#         \"ffr\": ffr,\n",
    "#         \"alignment\": alignment\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate overall metrics\n",
    "# overall_metrics = calculate_overall_metrics(data, best_assertions, assertion_results)\n",
    "# print(\"Overall metrics:\", json.dumps(overall_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate final report\n",
    "# report = {\n",
    "#     \"final_assertions\": best_assertions,\n",
    "#     \"assertion_metrics\": {assertion: metrics[assertion] for assertion in best_assertions.values()},\n",
    "#     \"overall_metrics\": overall_metrics\n",
    "# }\n",
    "\n",
    "# print(\"\\nFinal Report:\")\n",
    "# print(json.dumps(report, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we can improve the selecged aligned judges when a new batch of annotations come thru the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the weave.Scorer workflow work somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Dash post alignment\n",
    "# - Show big number of the final alignment \n",
    "# - The list of all assertions -> Weave.object (or Scorer\n",
    "# - MATCH THE SCORE FROM PAPER or do better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
